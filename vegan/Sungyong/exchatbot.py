import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
import streamlit as st

# FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë¡œë“œ í•¨ìˆ˜
def create_or_load_faiss_index(folder_path, faiss_file_path, chunk_size=1000, chunk_overlap=100):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    embeddings = OpenAIEmbeddings()

    if os.path.exists(faiss_file_path):
        vector_store = FAISS.load_local(faiss_file_path, embeddings, allow_dangerous_deserialization=True)
    else:
        all_docs = []
        for root, _, files in os.walk(folder_path):
            for file_name in files:
                if file_name.endswith(".pdf"):
                    file_path = os.path.join(root, file_name)
                    loader = PyPDFLoader(file_path)
                    documents = loader.load()
                    docs = text_splitter.split_documents(documents)
                    all_docs.extend(docs)

        vector_store = FAISS.from_documents(all_docs, embeddings)
        vector_store.save_local(faiss_file_path)
    return vector_store

# QA ì²´ì¸ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì • í•¨ìˆ˜
def create_qa_chain():
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ API í‚¤ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

    # ChatOpenAI ì´ˆê¸°í™”
    llm = ChatOpenAI(
        temperature=0,
        openai_api_key=api_key,       # OpenAI API í‚¤
        model_name="gpt-4o",         # gpt-4o ëª¨ë¸ ì§€ì •
        max_retries=3,                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        streaming=True,            # ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©
    )

    custom_prompt = PromptTemplate(
        input_variables=["context", "question", "history"],
        template=(
            "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ê²½ì œ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AIì…ë‹ˆë‹¤. "
            "ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ë³µì¡í•œ ê²½ì œ ì •ë³´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ìƒì„¸í•˜ê³  ì •í™•í•˜ë©° ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n"
            "ë‹¤ìŒì€ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n{context}\n\n"
            "ì´ì „ì— ë‚˜ëˆˆ ëŒ€í™”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{history}\n\n"
            "ìœ„ì˜ ì •ë³´ì™€ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²½ì œ ì „ë¬¸ê°€ë¡œì„œ "
            "ì‹¬ì¸µì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. "
            "ê°€ëŠ¥í•œ ê²½ìš°, êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ì„¤ëª…ì„ ì¶”ê°€í•˜ê³ , ê´€ë ¨ ë°°ê²½ ì§€ì‹ë„ í¬í•¨í•´ ì£¼ì„¸ìš”.\n\n"
            "ì§ˆë¬¸: {question}\n\n"
            "ì¹œì ˆí•˜ê³  ë¶„ì„ì ì¸ ë‹µë³€:"
        )
    )
    return create_stuff_documents_chain(llm, custom_prompt)

# ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ í´ë˜ìŠ¤
class ConversationHistory:
    def __init__(self):
        self.history = []

    def add_entry(self, question, answer):
        self.history.append({"question": question, "answer": answer})

    def to_text(self):
        return "\n".join(
            [f"Q: {entry['question']}\nA: {entry['answer']}" for entry in self.history]
        )

# Streamlitì—ì„œ ì‹¤í–‰ë  ì±—ë´‡ UI
def show():
    st.subheader("ğŸ¤– ê²½ì œ ì „ë¬¸ê°€ AI ì±—ë´‡")

    folder_path = "./reports"
    faiss_file_path = "./faiss_index"
    
    # ëŒ€í™” ê¸°ë¡ ë° QA ì²´ì¸ ì´ˆê¸°í™”
    # ëŒ€í™” ì €ì¥ ê³µê°„ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê²½ì œ ì „ë¬¸ê°€ AI ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì‹œë©´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."}]
    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if "history_manager" not in st.session_state:
        st.session_state.history_manager = ConversationHistory()

    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = create_qa_chain()

    if "vector_store" not in st.session_state:
        st.session_state.vector_store = create_or_load_faiss_index(folder_path, faiss_file_path)

    user_query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    # ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¶œë ¥
    if user_query:
        vector_store = st.session_state.vector_store
        qa_chain = st.session_state.qa_chain
        history_manager = st.session_state.history_manager

        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        try:
            retrieved_docs = vector_store.similarity_search(user_query, k=5)
            documents = [
                Document(
                    page_content=doc.page_content if hasattr(doc, 'page_content') else str(doc),
                    metadata=doc.metadata
                )
                for doc in retrieved_docs
            ]
            history_text = history_manager.to_text()

            # ìŠ¤íŠ¸ë¦¬ë°ëœ ë©”ì‹œì§€ ì²˜ë¦¬
            response = qa_chain.invoke({
                "context": documents,
                "question": user_query,
                "history": history_text
            })

            # ìŠ¤íŠ¸ë¦¬ë°ëœ ì‘ë‹µ ì²˜ë¦¬
            if isinstance(response, list):
                for chunk in response:
                    # ê° ì²­í¬ì˜ 'text' í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥
                    st.session_state.messages.append({"role": "assistant", "content": chunk['text']})
                    with st.chat_message("assistant"):
                        st.markdown(chunk['text'])
            else:
                # ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ” ì „ì²´ ì‘ë‹µì„ ì²˜ë¦¬
                st.session_state.messages.append({"role": "assistant", "content": response})
                with st.chat_message("assistant"):
                    st.markdown(response)

            history_manager.add_entry(user_query, response[-1]['text'] if isinstance(response, list) else response)
            st.session_state.history_manager = history_manager
        except Exception as e:
            response = f"ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.session_state.messages.append({"role": "assistant", "content": response})
            with st.chat_message("assistant"):
                st.markdown(response)

if __name__ == "__main__":
    show_chatbot()